@inproceedings{Fu,
    author = {Yu-Fu Fu and Jaehyuk Lee and Taesoo Kim},
    title = {autofz: Automated Fuzzer Composition at Runtime},
    booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
    year = {2023},
    isbn = {978-1-939133-37-3},
    address = {Anaheim, CA},
    pages = {1901--1918},
    url = {https://www.usenix.org/conference/usenixsecurity23/presentation/fu-yu-fu},
    publisher = {USENIX Association},
    month = aug
}

@article{Ecezia,
    title = {Improving fuzzing assessment methods through the analysis of metrics and experimental conditions},
    journal = {Computers & Security},
    volume = {124},
    pages = {102946},
    year = {2023},
    issn = {0167-4048},
    doi = {https://doi.org/10.1016/j.cose.2022.102946},
    url = {https://www.sciencedirect.com/science/article/pii/S0167404822003388},
    author = {Maialen Eceiza and Jose Luis Flores and Mikel Iturbe},
    keywords = {Fuzzing, Evaluation methodology, Security, Software testing, Metrics},
    abstract = {Fuzzing is nowadays one of the most widely used bug hunting techniques. By automatically generating malformed inputs, fuzzing aims to trigger unwanted behavior on its target. While fuzzing research has matured considerably in the last years, the evaluation and comparison of different fuzzing proposals remain challenging, as no standard set of metrics, data, or experimental conditions exist to allow such observation. This paper aims to fill that gap by proposing a standard set of features to allow such comparison. For that end, it first reviews the existing evaluation methods in the literature and discusses all existing metrics by evaluating seven fuzzers under identical experimental conditions. After examining the obtained results, it recommends a set of practices –particularly on the metrics to be used–, to allow proper comparison between different fuzzing proposals.}
}

@article{Miller, 
    author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan}, title = {An empirical study of the reliability of UNIX utilities}, year = {1990}, issue_date = {Dec. 1990}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {33}, number = {12}, issn = {0001-0782}, url = {https://doi.org/10.1145/96267.96279}, doi = {10.1145/96267.96279}, abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.}, journal = {Commun. ACM}, month = {dec}, pages = {32–44}, numpages = {13} 
}
